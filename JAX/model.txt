default_id_restore : (256,)
model.vit.embeddings.patch_embeddings.projection.kernel : (16, 16, 3, 768)
model.vit.embeddings.patch_embeddings.projection.bias : (768,)
model.vit.embeddings.position_embeddings.pos_embedding : (1, 257, 768)
model.vit.embeddings.cls_token : (1, 1, 768)
model.vit.encoder.layers.0.layernorm_before.scale : (768,)
model.vit.encoder.layers.0.layernorm_before.bias : (768,)
model.vit.encoder.layers.0.attention.attention.query.kernel : (768, 12, 64)
model.vit.encoder.layers.0.attention.attention.query.bias : (12, 64)
model.vit.encoder.layers.0.attention.attention.key.kernel : (768, 12, 64)
model.vit.encoder.layers.0.attention.attention.key.bias : (12, 64)
model.vit.encoder.layers.0.attention.attention.value.kernel : (768, 12, 64)
model.vit.encoder.layers.0.attention.attention.value.bias : (12, 64)
model.vit.encoder.layers.0.attention.attention.dense.kernel : (12, 64, 768)
model.vit.encoder.layers.0.attention.attention.dense.bias : (768,)
model.vit.encoder.layers.0.layernorm_after.scale : (768,)
model.vit.encoder.layers.0.layernorm_after.bias : (768,)
model.vit.encoder.layers.0.intermediate.dense.kernel : (768, 3072)
model.vit.encoder.layers.0.intermediate.dense.bias : (3072,)
model.vit.encoder.layers.0.output.dense.kernel : (3072, 768)
model.vit.encoder.layers.0.output.dense.bias : (768,)




model.vit.encoder.layers.1.layernorm_before.scale : (768,)
model.vit.encoder.layers.1.layernorm_before.bias : (768,)
model.vit.encoder.layers.1.attention.attention.query.kernel : (768, 12, 64)
model.vit.encoder.layers.1.attention.attention.query.bias : (12, 64)
model.vit.encoder.layers.1.attention.attention.key.kernel : (768, 12, 64)
model.vit.encoder.layers.1.attention.attention.key.bias : (12, 64)
model.vit.encoder.layers.1.attention.attention.value.kernel : (768, 12, 64)
model.vit.encoder.layers.1.attention.attention.value.bias : (12, 64)
model.vit.encoder.layers.1.attention.attention.dense.kernel : (12, 64, 768)
model.vit.encoder.layers.1.attention.attention.dense.bias : (768,)
model.vit.encoder.layers.1.layernorm_after.scale : (768,)
model.vit.encoder.layers.1.layernorm_after.bias : (768,)
model.vit.encoder.layers.1.intermediate.dense.kernel : (768, 3072)
model.vit.encoder.layers.1.intermediate.dense.bias : (3072,)
model.vit.encoder.layers.1.output.dense.kernel : (3072, 768)
model.vit.encoder.layers.1.output.dense.bias : (768,)
model.vit.layernorm.scale : (768,)
model.vit.layernorm.bias : (768,)
model.decoder.decoder_embed.kernel : (768, 512)
model.decoder.decoder_embed.bias : (512,)
model.decoder.mask_token : (1, 1, 512)
model.decoder.trainable_cls_token : (1, 1, 512)
model.decoder_pos_embed.pos_embedding : (1, 257, 512)
model.decoder.decoder_layer.0.layernorm_before.scale : (512,)
model.decoder.decoder_layer.0.layernorm_before.bias : (512,)
model.decoder.decoder_layer.0.attention.attention.query.kernel : (512, 16, 32)
model.decoder.decoder_layer.0.attention.attention.query.bias : (16, 32)
model.decoder.decoder_layer.0.attention.attention.key.kernel : (512, 16, 32)
model.decoder.decoder_layer.0.attention.attention.key.bias : (16, 32)
model.decoder.decoder_layer.0.attention.attention.value.kernel : (512, 16, 32)
model.decoder.decoder_layer.0.attention.attention.value.bias : (16, 32)
model.decoder.decoder_layer.0.attention.attention.dense.kernel : (16, 32, 512)
model.decoder.decoder_layer.0.attention.attention.dense.bias : (512,)
model.decoder.decoder_layer.0.layernorm_after.scale : (512,)
model.decoder.decoder_layer.0.layernorm_after.bias : (512,)
model.decoder.decoder_layer.0.intermediate.dense.kernel : (512, 2048)
model.decoder.decoder_layer.0.intermediate.dense.bias : (2048,)
model.decoder.decoder_layer.0.output.dense.kernel : (2048, 512)
model.decoder.decoder_layer.0.output.dense.bias : (512,)
model.decoder.decoder_layer.1.layernorm_before.scale : (512,)
model.decoder.decoder_layer.1.layernorm_before.bias : (512,)
model.decoder.decoder_layer.1.attention.attention.query.kernel : (512, 16, 32)
model.decoder.decoder_layer.1.attention.attention.query.bias : (16, 32)
model.decoder.decoder_layer.1.attention.attention.key.kernel : (512, 16, 32)
model.decoder.decoder_layer.1.attention.attention.key.bias : (16, 32)
model.decoder.decoder_layer.1.attention.attention.value.kernel : (512, 16, 32)
model.decoder.decoder_layer.1.attention.attention.value.bias : (16, 32)
model.decoder.decoder_layer.1.attention.attention.dense.kernel : (16, 32, 512)
model.decoder.decoder_layer.1.attention.attention.dense.bias : (512,)
model.decoder.decoder_layer.1.layernorm_after.scale : (512,)
model.decoder.decoder_layer.1.layernorm_after.bias : (512,)
model.decoder.decoder_layer.1.intermediate.dense.kernel : (512, 2048)
model.decoder.decoder_layer.1.intermediate.dense.bias : (2048,)
model.decoder.decoder_layer.1.output.dense.kernel : (2048, 512)
model.decoder.decoder_layer.1.output.dense.bias : (512,)
model.decoder.decoder_norm.scale : (512,)
model.decoder.decoder_norm.bias : (512,)
model.decoder.decoder_pred.kernel : (512, 768)
model.decoder.decoder_pred.bias : (768,)
