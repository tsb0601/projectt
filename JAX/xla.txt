Stage1MAE model loaded with mean [0.485, 0.456, 0.406] and std [0.229, 0.224, 0.225], mask ratio 0.0
stage1 model: Stage1ModelWrapper(
  (stage_1_model): Stage1MAE(
    (model): ViTMAEForPreTraining(
      (vit): ViTMAEModel(
        (embeddings): ViTMAEEmbeddings(
          (patch_embeddings): ViTMAEPatchEmbeddings(
            (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          )
        )
        (encoder): ViTMAEEncoder(
          (layer): ModuleList(
            (0-11): 12 x ViTMAELayer(
              (attention): ViTMAEAttention(
                (attention): ViTMAESelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
                (output): ViTMAESelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (intermediate): ViTMAEIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): ViTMAEOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            )
          )
        )
        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): ViTMAEDecoder(
        (decoder_embed): Linear(in_features=768, out_features=512, bias=True)
        (decoder_layers): ModuleList(
          (0-7): 8 x ViTMAELayer(
            (attention): ViTMAEAttention(
              (attention): ViTMAESelfAttention(
                (query): Linear(in_features=512, out_features=512, bias=True)
                (key): Linear(in_features=512, out_features=512, bias=True)
                (value): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): ViTMAESelfOutput(
                (dense): Linear(in_features=512, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (intermediate): ViTMAEIntermediate(
              (dense): Linear(in_features=512, out_features=2048, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): ViTMAEOutput(
              (dense): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          )
        )
        (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
      )
    )
  )
  (connector): ReshapeAndSplit_connector()
)
image_mean: shape=torch.Size([1, 3, 1, 1])
image_std: shape=torch.Size([1, 3, 1, 1])
noise: shape=torch.Size([256])
default_id_restore: shape=torch.Size([256])
model.vit.embeddings.cls_token: shape=torch.Size([1, 1, 768])
model.vit.embeddings.position_embeddings: shape=torch.Size([1, 257, 768])
model.vit.embeddings.patch_embeddings.projection.weight: shape=torch.Size([768, 3, 16, 16])
model.vit.embeddings.patch_embeddings.projection.bias: shape=torch.Size([768])


model.vit.encoder.layer.0.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.0.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.0.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.0.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.0.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.0.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.0.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.0.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.0.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.0.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.0.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.0.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.0.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.0.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.0.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.0.layernorm_after.bias: shape=torch.Size([768])



model.vit.encoder.layer.1.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.1.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.1.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.1.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.1.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.1.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.1.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.1.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.1.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.1.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.1.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.1.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.1.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.1.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.1.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.1.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.2.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.2.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.2.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.2.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.2.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.2.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.2.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.2.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.2.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.2.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.3.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.3.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.3.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.3.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.3.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.3.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.3.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.3.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.3.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.3.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.4.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.4.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.4.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.4.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.4.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.4.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.4.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.4.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.4.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.4.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.5.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.5.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.5.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.5.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.5.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.5.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.5.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.5.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.5.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.5.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.6.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.6.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.6.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.6.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.6.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.6.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.6.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.6.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.6.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.6.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.7.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.7.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.7.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.7.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.7.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.7.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.7.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.7.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.7.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.7.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.8.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.8.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.8.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.8.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.8.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.8.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.8.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.8.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.8.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.8.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.9.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.9.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.9.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.9.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.9.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.9.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.9.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.9.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.9.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.9.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.10.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.10.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.10.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.10.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.10.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.10.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.10.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.10.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.10.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.10.layernorm_after.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.attention.attention.query.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.11.attention.attention.query.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.attention.attention.key.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.11.attention.attention.key.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.attention.attention.value.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.11.attention.attention.value.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.attention.output.dense.weight: shape=torch.Size([768, 768])
model.vit.encoder.layer.11.attention.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.intermediate.dense.weight: shape=torch.Size([3072, 768])
model.vit.encoder.layer.11.intermediate.dense.bias: shape=torch.Size([3072])
model.vit.encoder.layer.11.output.dense.weight: shape=torch.Size([768, 3072])
model.vit.encoder.layer.11.output.dense.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.layernorm_before.weight: shape=torch.Size([768])
model.vit.encoder.layer.11.layernorm_before.bias: shape=torch.Size([768])
model.vit.encoder.layer.11.layernorm_after.weight: shape=torch.Size([768])
model.vit.encoder.layer.11.layernorm_after.bias: shape=torch.Size([768])
model.vit.layernorm.weight: shape=torch.Size([768])
model.vit.layernorm.bias: shape=torch.Size([768])
model.decoder.mask_token: shape=torch.Size([1, 1, 512])
model.decoder.decoder_pos_embed: shape=torch.Size([1, 257, 512])
model.decoder.trainable_cls_token: shape=torch.Size([1, 1, 512])
model.decoder.decoder_embed.weight: shape=torch.Size([512, 768])
model.decoder.decoder_embed.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.0.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.0.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.0.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.0.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.0.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.0.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.0.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.0.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.0.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.0.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.1.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.1.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.1.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.1.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.1.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.1.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.1.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.1.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.1.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.1.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.2.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.2.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.2.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.2.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.2.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.2.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.2.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.2.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.2.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.2.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.3.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.3.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.3.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.3.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.3.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.3.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.3.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.3.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.3.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.3.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.4.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.4.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.4.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.4.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.4.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.4.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.4.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.4.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.4.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.4.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.5.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.5.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.5.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.5.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.5.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.5.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.5.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.5.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.5.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.5.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.6.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.6.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.6.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.6.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.6.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.6.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.6.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.6.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.6.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.6.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.attention.attention.query.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.7.attention.attention.query.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.attention.attention.key.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.7.attention.attention.key.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.attention.attention.value.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.7.attention.attention.value.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.attention.output.dense.weight: shape=torch.Size([512, 512])
model.decoder.decoder_layers.7.attention.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.intermediate.dense.weight: shape=torch.Size([2048, 512])
model.decoder.decoder_layers.7.intermediate.dense.bias: shape=torch.Size([2048])
model.decoder.decoder_layers.7.output.dense.weight: shape=torch.Size([512, 2048])
model.decoder.decoder_layers.7.output.dense.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.layernorm_before.weight: shape=torch.Size([512])
model.decoder.decoder_layers.7.layernorm_before.bias: shape=torch.Size([512])
model.decoder.decoder_layers.7.layernorm_after.weight: shape=torch.Size([512])
model.decoder.decoder_layers.7.layernorm_after.bias: shape=torch.Size([512])
model.decoder.decoder_norm.weight: shape=torch.Size([512])
model.decoder.decoder_norm.bias: shape=torch.Size([512])
model.decoder.decoder_pred.weight: shape=torch.Size([768, 512])
model.decoder.decoder_pred.bias: shape=torch.Size([768])
